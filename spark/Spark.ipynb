{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import the required libraries\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Step 1: Create a Spark Configuration and Spark Context\n",
        "conf = SparkConf().setAppName(\"SparkBasics\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Step 2: Create an RDD from a list\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Step 3: Perform operations on the RDD\n",
        "# Example: Multiply each element by 2\n",
        "mapped_rdd = rdd.map(lambda x: x * 2)\n",
        "\n",
        "# Collect the result\n",
        "result = mapped_rdd.collect()\n",
        "\n",
        "# Step 4: Print the result\n",
        "print(\"Original Data:\", data)\n",
        "print(\"Transformed Data (multiplied by 2):\", result)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1TF5CO9yXq4",
        "outputId": "e6c0c3a1-2bd5-406c-ee81-749abd456ba5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data: [1, 2, 3, 4, 5]\n",
            "Transformed Data (multiplied by 2): [2, 4, 6, 8, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Step 1: Set up SparkContext\n",
        "conf = SparkConf().setAppName(\"MinMaxTemperature\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Step 2: Sample temperature data in the form (Location, Temperature)\n",
        "data = [\n",
        "    (\"New York\", 30),\n",
        "    (\"Los Angeles\", 25),\n",
        "    (\"New York\", 28),\n",
        "    (\"Los Angeles\", 22),\n",
        "    (\"New York\", 35),\n",
        "    (\"Los Angeles\", 20),\n",
        "    (\"Chicago\", 15),\n",
        "    (\"Chicago\", 10),\n",
        "    (\"Chicago\", 12)\n",
        "]\n",
        "\n",
        "# Step 3: Create an RDD from the data\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Step 4: Group data by location and calculate both min and max temperatures for each location\n",
        "grouped_rdd = rdd.groupByKey()\n",
        "\n",
        "# Find both minimum and maximum temperatures\n",
        "min_max_temp_rdd = grouped_rdd.mapValues(lambda temps: (min(temps), max(temps)))\n",
        "\n",
        "# Step 5: Collect the results and display\n",
        "result = min_max_temp_rdd.collect()\n",
        "print(\"Minimum and Maximum Temperatures by Location:\")\n",
        "for location, (min_temp, max_temp) in result:\n",
        "    print(f\"{location}: Min = {min_temp}°C, Max = {max_temp}°C\")\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krfmlmGL2upQ",
        "outputId": "780fe937-ecfc-4b29-cdd7-4a5e6292d2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum and Maximum Temperatures by Location:\n",
            "New York: Min = 28°C, Max = 35°C\n",
            "Los Angeles: Min = 20°C, Max = 25°C\n",
            "Chicago: Min = 10°C, Max = 15°C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split, col, count\n",
        "\n",
        "# Step 1: Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"WordCountDataFrame\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 2: Create a DataFrame with text data\n",
        "data = [(\"Hello world spark spark\",), (\"Hello again spark\",), (\"Welcome to the world of spark\",)]\n",
        "columns = [\"line\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Step 3: Split lines into words, explode to create one word per row\n",
        "words_df = df.select(explode(split(col(\"line\"), \" \")).alias(\"word\"))\n",
        "\n",
        "# Step 4: Count occurrences of each word\n",
        "word_count_df = words_df.groupBy(\"word\").agg(count(\"word\").alias(\"count\"))\n",
        "\n",
        "# Step 5: Show the result\n",
        "word_count_df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApztCUYk30RC",
        "outputId": "b616e772-4d63-4524-a00e-3d1fb2b15d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|   word|count|\n",
            "+-------+-----+\n",
            "|  Hello|    2|\n",
            "|  spark|    4|\n",
            "|  world|    2|\n",
            "|  again|    1|\n",
            "|    the|    1|\n",
            "|     of|    1|\n",
            "|Welcome|    1|\n",
            "|     to|    1|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg, sum, col\n",
        "\n",
        "# Step 1: Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SQLCommandsOnDataFrame\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 2: Create a DataFrame with sample data\n",
        "data = [\n",
        "    (\"Alice\", \"Electronics\", 200),\n",
        "    (\"Bob\", \"Electronics\", 150),\n",
        "    (\"Alice\", \"Clothing\", 100),\n",
        "    (\"Bob\", \"Clothing\", 50),\n",
        "    (\"Alice\", \"Electronics\", 300)\n",
        "]\n",
        "columns = [\"Name\", \"Category\", \"Amount\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Step 3: Execute SQL-style functions\n",
        "# Calculate total and average spending by category\n",
        "result_df = df.groupBy(\"Category\") \\\n",
        "    .agg(\n",
        "        sum(\"Amount\").alias(\"TotalSpent\"),\n",
        "        avg(\"Amount\").alias(\"AverageSpent\")\n",
        "    )\n",
        "\n",
        "# Step 4: Show the result\n",
        "result_df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nytEtmI7TxXp",
        "outputId": "c4b2bdbe-ac81-4083-e786-40796ce32a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+------------------+\n",
            "|   Category|TotalSpent|      AverageSpent|\n",
            "+-----------+----------+------------------+\n",
            "|Electronics|       650|216.66666666666666|\n",
            "|   Clothing|       150|              75.0|\n",
            "+-----------+----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum\n",
        "\n",
        "# Step 1: Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TotalSpentByCustomer\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 2: Create a DataFrame with sample data\n",
        "data = [\n",
        "    (\"Alice\", 200),\n",
        "    (\"Bob\", 150),\n",
        "    (\"Alice\", 100),\n",
        "    (\"Bob\", 50),\n",
        "    (\"Alice\", 300)\n",
        "]\n",
        "columns = [\"Customer\", \"Amount\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Step 3: Group by Customer and calculate the total amount spent\n",
        "result_df = df.groupBy(\"Customer\").agg(sum(\"Amount\").alias(\"TotalSpent\"))\n",
        "\n",
        "# Step 4: Show the result\n",
        "result_df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq4v8p4sUg1A",
        "outputId": "c4da5aa8-0383-45d6-e1be-7d50f6f3c48f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+\n",
            "|Customer|TotalSpent|\n",
            "+--------+----------+\n",
            "|     Bob|       200|\n",
            "|   Alice|       600|\n",
            "+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Step 1: Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BroadcastVariablesExample\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 2: Create a DataFrame with movie IDs and ratings\n",
        "movie_data = [\n",
        "    (1, 4.5),\n",
        "    (2, 3.0),\n",
        "    (1, 5.0),\n",
        "    (3, 4.0)\n",
        "]\n",
        "columns = [\"MovieID\", \"Rating\"]\n",
        "ratings_df = spark.createDataFrame(movie_data, columns)\n",
        "\n",
        "# Step 3: Create a dictionary with movie names\n",
        "movie_names = {\n",
        "    1: \"Inception\",\n",
        "    2: \"Interstellar\",\n",
        "    3: \"The Dark Knight\"\n",
        "}\n",
        "\n",
        "# Step 4: Broadcast the movie names dictionary\n",
        "movie_names_broadcast = spark.sparkContext.broadcast(movie_names)\n",
        "\n",
        "# Step 5: Map MovieID to Movie Name using the broadcast variable\n",
        "def map_movie_name(movie_id):\n",
        "    return movie_names_broadcast.value.get(movie_id, \"Unknown\")\n",
        "\n",
        "# Register the function as a UDF\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "map_movie_name_udf = udf(map_movie_name, StringType())\n",
        "\n",
        "# Add a new column with movie names\n",
        "result_df = ratings_df.withColumn(\"MovieName\", map_movie_name_udf(ratings_df.MovieID))\n",
        "\n",
        "# Step 6: Show the result\n",
        "result_df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rd1eUhsUnNz",
        "outputId": "ebbb05a9-3400-4c9f-f2c2-878210b2a85a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+---------------+\n",
            "|MovieID|Rating|      MovieName|\n",
            "+-------+------+---------------+\n",
            "|      1|   4.5|      Inception|\n",
            "|      2|   3.0|   Interstellar|\n",
            "|      1|   5.0|      Inception|\n",
            "|      3|   4.0|The Dark Knight|\n",
            "+-------+------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql.types import IntegerType, FloatType\n",
        "\n",
        "# Step 1: Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MovieRecommendations\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 2: Create a DataFrame with sample user-movie ratings\n",
        "data = [\n",
        "    (0, 1, 4.0),  # User 0 rated Movie 1 with 4.0\n",
        "    (0, 2, 2.0),  # User 0 rated Movie 2 with 2.0\n",
        "    (1, 2, 5.0),  # User 1 rated Movie 2 with 5.0\n",
        "    (1, 3, 3.0),  # User 1 rated Movie 3 with 3.0\n",
        "    (2, 1, 1.0),  # User 2 rated Movie 1 with 1.0\n",
        "    (2, 3, 4.0)   # User 2 rated Movie 3 with 4.0\n",
        "]\n",
        "columns = [\"UserID\", \"MovieID\", \"Rating\"]\n",
        "ratings_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Step 3: Create an ALS model\n",
        "als = ALS(\n",
        "    maxIter=5,  # Number of iterations\n",
        "    regParam=0.01,  # Regularization parameter\n",
        "    userCol=\"UserID\",  # Column for users\n",
        "    itemCol=\"MovieID\",  # Column for items\n",
        "    ratingCol=\"Rating\",  # Column for ratings\n",
        "    coldStartStrategy=\"drop\"  # Drop rows with null predictions\n",
        ")\n",
        "\n",
        "# Step 4: Train the ALS model\n",
        "model = als.fit(ratings_df)\n",
        "\n",
        "# Step 5: Generate top 2 movie recommendations for each user\n",
        "user_recommendations = model.recommendForAllUsers(2)\n",
        "\n",
        "# Step 6: Show the recommendations\n",
        "user_recommendations.show(truncate=False)\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieGAkBDiUxLM",
        "outputId": "743263a8-9750-44ab-fe57-d867deab3987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------------------------+\n",
            "|UserID|recommendations                |\n",
            "+------+-------------------------------+\n",
            "|0     |[{1, 3.994191}, {2, 2.0000634}]|\n",
            "|1     |[{2, 4.996521}, {3, 3.0001428}]|\n",
            "|2     |[{3, 3.9961724}, {2, 3.443232}]|\n",
            "+------+-------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, desc\n",
        "\n",
        "# Step 1: Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MostViewedURL\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 2: Create a DataFrame with sample log data\n",
        "data = [\n",
        "    (1, \"http://example.com/home\"),\n",
        "    (2, \"http://example.com/about\"),\n",
        "    (3, \"http://example.com/home\"),\n",
        "    (4, \"http://example.com/contact\"),\n",
        "    (5, \"http://example.com/home\"),\n",
        "    (6, \"http://example.com/about\"),\n",
        "    (7, \"http://example.com/contact\"),\n",
        "    (8, \"http://example.com/home\")\n",
        "]\n",
        "columns = [\"UserID\", \"URL\"]\n",
        "logs_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Step 3: Count the number of views for each URL\n",
        "url_counts_df = logs_df.groupBy(\"URL\").agg(count(\"URL\").alias(\"ViewCount\"))\n",
        "\n",
        "# Step 4: Sort the URLs by the number of views in descending order\n",
        "sorted_url_counts_df = url_counts_df.orderBy(desc(\"ViewCount\"))\n",
        "\n",
        "# Step 5: Show the most viewed URLs\n",
        "sorted_url_counts_df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcLuUEsIU4Bi",
        "outputId": "3907a1bb-3000-464a-e774-a53d414a2729"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------+\n",
            "|                 URL|ViewCount|\n",
            "+--------------------+---------+\n",
            "|http://example.co...|        4|\n",
            "|http://example.co...|        2|\n",
            "|http://example.co...|        2|\n",
            "+--------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2DryOnKdYKJM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}