# -*- coding: utf-8 -*-
"""Sparkfile.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_zIf5hIemcI_DuXXuizTZShbu6bjt1-2
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import min, max

# Step 1: Create a SparkSession
spark = SparkSession.builder \
    .appName("MinMaxTemperatureCSV") \
    .master("local[*]") \
    .getOrCreate()

# Step 2: Read the CSV file into a DataFrame
file_path = "temperature_data.csv"  # Replace with your file path
df = spark.read.csv(file_path, header=True, inferSchema=True)

# Step 3: Group by "Location" and calculate min and max temperatures
result_df = df.groupBy("Location").agg(
    min("Temperature").alias("MinTemperature"),
    max("Temperature").alias("MaxTemperature")
)

# Step 4: Show the result
result_df.show()

# Stop the SparkSession
spark.stop()

from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, split, col, count

# Step 1: Create a SparkSession
spark = SparkSession.builder \
    .appName("WordCountCSV") \
    .master("local[*]") \
    .getOrCreate()

# Step 2: Read the CSV file into a DataFrame
file_path = "text_data.csv"  # Replace with your file path
df = spark.read.csv(file_path, header=True, inferSchema=True)

# Step 3: Split lines into words, explode to create one word per row
words_df = df.select(explode(split(col("line"), " ")).alias("word"))

# Step 4: Count occurrences of each word
word_count_df = words_df.groupBy("word").agg(count("word").alias("count"))

# Step 5: Show the result
word_count_df.show()

# Stop the SparkSession
spark.stop()

from pyspark.sql import SparkSession
from pyspark.sql.functions import avg, sum

# Step 1: Create a SparkSession
spark = SparkSession.builder \
    .appName("SQLCommandsOnCSV") \
    .master("local[*]") \
    .getOrCreate()

# Step 2: Read the CSV file into a DataFrame
file_path = "spending_data.csv"  # Replace with your file path
df = spark.read.csv(file_path, header=True, inferSchema=True)

# Step 3: Execute SQL-style functions
# Calculate total and average spending by category
result_df = df.groupBy("Category") \
    .agg(
        sum("Amount").alias("TotalSpent"),
        avg("Amount").alias("AverageSpent")
    )

# Step 4: Show the result
result_df.show()

# Stop the SparkSession
spark.stop()

from pyspark.sql import SparkSession
from pyspark.sql.functions import sum

# Step 1: Create a SparkSession
spark = SparkSession.builder \
    .appName("TotalSpentByCustomerCSV") \
    .master("local[*]") \
    .getOrCreate()

# Step 2: Read the CSV file into a DataFrame
file_path = "customer_data.csv"  # Replace with your file path
df = spark.read.csv(file_path, header=True, inferSchema=True)

# Step 3: Group by Customer and calculate the total amount spent
result_df = df.groupBy("Customer").agg(sum("Amount").alias("TotalSpent"))

# Step 4: Show the result
result_df.show()

# Stop the SparkSession
spark.stop()

from pyspark.sql import SparkSession

# Step 1: Create a SparkSession
spark = SparkSession.builder \
    .appName("BroadcastVariablesCSV") \
    .master("local[*]") \
    .getOrCreate()

# Step 2: Read the movies and ratings files into DataFrames
movies_file = "movies.csv"  # Replace with your file path
ratings_file = "ratings.csv"  # Replace with your file path

movies_df = spark.read.csv(movies_file, header=True, inferSchema=True)
ratings_df = spark.read.csv(ratings_file, header=True, inferSchema=True)

# Step 3: Create a dictionary from the movies DataFrame
movie_names = {row["MovieID"]: row["MovieName"] for row in movies_df.collect()}

# Step 4: Broadcast the movie names dictionary
movie_names_broadcast = spark.sparkContext.broadcast(movie_names)

# Step 5: Map MovieID to Movie Name using the broadcast variable
def map_movie_name(movie_id):
    return movie_names_broadcast.value.get(movie_id, "Unknown")

# Register the function as a UDF
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

map_movie_name_udf = udf(map_movie_name, StringType())

# Add a new column with movie names
result_df = ratings_df.withColumn("MovieName", map_movie_name_udf(ratings_df.MovieID))

# Step 6: Show the result
result_df.show()

# Stop the SparkSession
spark.stop()

from pyspark.sql import SparkSession
from pyspark.ml.recommendation import ALS

# Step 1: Create a SparkSession
spark = SparkSession.builder \
    .appName("MovieRecommendationsFromCSV") \
    .master("local[*]") \
    .getOrCreate()

# Step 2: Read the ratings file into a DataFrame
ratings_file = "ratings.csv"  # Replace with your file path
ratings_df = spark.read.csv(ratings_file, header=True, inferSchema=True)

# Step 3: Create an ALS model
als = ALS(
    maxIter=5,  # Number of iterations
    regParam=0.01,  # Regularization parameter
    userCol="UserID",  # Column for users
    itemCol="MovieID",  # Column for items
    ratingCol="Rating",  # Column for ratings
    coldStartStrategy="drop"  # Drop rows with null predictions
)

# Step 4: Train the ALS model
model = als.fit(ratings_df)

# Step 5: Generate top 2 movie recommendations for each user
user_recommendations = model.recommendForAllUsers(2)

# Step 6: Show the recommendations
user_recommendations.show(truncate=False)

# Stop the SparkSession
spark.stop()

from pyspark.sql import SparkSession
from pyspark.sql.functions import count, desc

# Step 1: Create a SparkSession
spark = SparkSession.builder \
    .appName("MostViewedURLFromCSV") \
    .master("local[*]") \
    .getOrCreate()

# Step 2: Read the log file into a DataFrame
file_path = "web_logs.csv"  # Replace with your file path
logs_df = spark.read.csv(file_path, header=True, inferSchema=True)

# Step 3: Count the number of views for each URL
url_counts_df = logs_df.groupBy("URL").agg(count("URL").alias("ViewCount"))

# Step 4: Sort the URLs by the number of views in descending order
sorted_url_counts_df = url_counts_df.orderBy(desc("ViewCount"))

# Step 5: Show the most viewed URLs
sorted_url_counts_df.show()

# Stop the SparkSession
spark.stop()